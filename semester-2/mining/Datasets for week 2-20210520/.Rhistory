kable_styling(font_size = 10, latex_options = "hold_position")
skim_with(numeric = list(hist = NULL, missing = NULL, complete = NULL))
Cred %>%
skim_to_list() %>%
.$numeric %>%
kable(col.names = c("Variable", "n", "Mean", "SD", "Minimum", "1st quartile", "Median",
"3rd quartile","Maximum"), caption =
'\\label{tab:summary} Summary statistics on Credit Card Balance, Income and Age.',
booktabs = TRUE, format = "latex") %>%
kable_styling(font_size = 10, latex_options = "hold_position")
ggplot(Cred, aes(x = Age, y = Balance)) +
geom_point() +
labs(x = "Age (in years)", y = "Credit card balance (in $)",
title = "Relationship between balance and age") +
geom_smooth(method = "lm", se = FALSE)
Balance.model <- lm(Balance ~ Age + Income, data = Cred)
get_regression_table(Balance.model) %>%
kable(caption =
'\\label{tab:reg} Estimated Coefficients from the fitted model
Balance = Age + Income ', booktabs = TRUE, format = "latex") %>%
kable_styling(font_size = 10, latex_options = "hold_position")
regression.points <- get_regression_points(Balance.model)
ggplot(regression.points, aes(x = Income, y = residual)) +
geom_point() +
labs(x = "Income (in $1000)", y = "Residual", title = "Residuals vs income") +
geom_hline(yintercept = 0, col = "blue", size = 1)
ggplot(regression.points, aes(x = Age, y = residual)) +
geom_point() +
labs(x = "Age (in years)", y = "Residual", title = "Residuals vs age") +
geom_hline(yintercept = 0, col = "blue", size = 1)
s
ggplot(regression.points, aes(x = residual)) +
geom_histogram(color = "white") +
labs(x = "Residual")
#3. Return to the Credit data set and fit a parallel regression lines model with Balance as the outcome
#variable, and Income and Student as the explanatory variables, respectively. Assess the assumptions of
#the fitted model.
Cred <- Credit %>%
select(Balance, Income, Student)
# Cred %>%
# skim()
Cred %>%
group_by(Student) %>%
summarise(n()) %>%
kable(col.names = c("Student", "n"),caption =
'\\label{tab:T3Student} Numbers of students and non-students',
booktabs = TRUE, format = "latex") %>%
kable_styling(font_size = 10, latex_options = "hold_position")
# Cred %>%
# skim()
Cred %>%
group_by(Student) %>%
summarise(n()) %>%
kable(col.names = c("Student", "n"),caption =
'\\label{tab:T3Student} Numbers of students and non-students',
booktabs = TRUE, format = "latex") %>%
kable_styling(font_size = 10, latex_options = "hold_position")
Cred$Balance <- as.numeric(Cred$Balance)
skim_with(numeric = list(hist = NULL, missing = NULL, complete = NULL))
Cred %>%
skim_to_list() %>%
.$numeric %>%
kable(col.names = c("Variable", "n", "Mean", "SD", "Minimum", "1st quartile", "Median",
"3rd quartile","Maximum"), caption =
'\\label{tab:T3summary} Summary statistics on Credit Card Balance and Income.',
booktabs = TRUE, format = "latex") %>%
kable_styling(font_size = 10, latex_options = "hold_position")
library(ggplot2)
library(dplyr)
library(moderndive)
library(ISLR)
library(skimr)
library(plotly)
library(tidyr)
library(jtools)
Cred %>%
skim_to_list() %>%
.$numeric %>%
kable(col.names = c("Variable", "n", "Mean", "SD", "Minimum", "1st quartile", "Median",
"3rd quartile","Maximum"), caption =
'\\label{tab:T3summary} Summary statistics on Credit Card Balance and Income.',
booktabs = TRUE, format = "latex") %>%
kable_styling(font_size = 10, latex_options = "hold_position")
ggplot(Cred, aes(x = Income, y = Balance, color = Student)) +
geom_jitter() +
labs(x = "Income (in $1000)", y = "Credit card balance (in $)", color = "Student") +
geom_smooth(method = "lm", se = FALSE)
par.model <- lm(Balance ~ Income + Student, data = Cred)
get_regression_table(par.model) %>%
kable(caption =
'\\label{tab:T3reg} Estimated Coefficients from the fitted model
Balance = Income + Student', booktabs = TRUE, format = "latex") %>%
kable_styling(font_size = 10, latex_options = "hold_position")
regression.points <- get_regression_points(par.model)
ggplot(regression.points, aes(x = Income, y = residual)) +
geom_point() +
labs(x = "Income (in $1000)", y = "Residual") +
geom_hline(yintercept = 0, col = "blue", size = 1) +
facet_wrap(~ Student)
et_regression_table(par.model) %>%
kable(caption =
'\\label{tab:T3reg} Estimated Coefficients from the fitted model
Balance = Income + Student', booktabs = TRUE, format = "latex")
get_regression_table(par.model) %>%
kable(caption =
'\\label{tab:T3reg} Estimated Coefficients from the fitted model
Balance = Income + Student', booktabs = TRUE, format = "latex")
get_regression_table(par.model) %>%
kable(caption =
'\\label{tab:T3reg} Estimated Coefficients from the fitted model
Balance = Income + Student', booktabs = TRUE, format = "latex")
get_regression_table(par.model) %>%
kable(caption =
'\\label{tab:T3reg} Estimated Coefficients from the fitted model
Balance = Income + Student', booktabs = TRUE, format = "latex") %>%
kable_styling(font_size = 10, latex_options = "hold_position")
regression.points <- get_regression_points(par.model)
ggplot(regression.points, aes(x = Income, y = residual)) +
geom_point() +
labs(x = "Income (in $1000)", y = "Residual") +
geom_hline(yintercept = 0, col = "blue", size = 1) +
facet_wrap(~ Student)
ggplot(regression.points, aes(x = Balance_hat, y = residual)) +
geom_point() +
labs(x = "Fitted values", y = "Residual") +
geom_hline(yintercept = 0, col = "blue", size = 1) +
facet_wrap(~ Student)
ggplot(regression.points, aes(x = residual)) +
geom_histogram(color = "white") +
labs(x = "Residual") +
facet_wrap(~Student)
library(datasets)
Irs <- iris %>%
select(Sepal.Width, Sepal.Length, Species)
#Trickier
#4. Load the library datasets and look at the iris data set of Edgar Anderson containing measurements
#(in centimetres) on 150 different flowers across three different species of iris. Fit an interaction model with
#Sepal.Width as the outcome variable, and Sepal.Length and Species as the explanatory variables.
#Assess the assumptions of the fitted model.
library(datasets)
Irs <- iris %>%
select(Sepal.Width, Sepal.Length, Species)
Irs %>%
skim()
Irs %>%
group_by(Species) %>%
summarise(n()) %>%
kable(col.names = c("Species", "n"),caption =
'\\label{tab:T4Species} Numbers of different species',
booktabs = TRUE, format = "latex") %>%
kable_styling(font_size = 10, latex_options = "hold_position")
skim_with(numeric = list(hist = NULL, missing = NULL, complete = NULL))
Irs %>%
skim_to_list() %>%
.$numeric %>%
kable(col.names = c("Variable", "n", "Mean", "SD", "Minimum", "1st quartile", "Median",
"3rd quartile","Maximum"), caption =
'\\label{tab:T4summary} Summary statistics on Iris variables.',
booktabs = TRUE, format = "latex") %>%
kable_styling(font_size = 10, latex_options = "hold_position")
Irs %>%
skim_to_list()
Irs %>%
skim_to_list() %>%
.$numeric
Irs %>%
skim_to_list() %>%
.$numeric %>%
kable(col.names = c("Variable", "n", "Mean", "SD", "Minimum", "1st quartile", "Median",
"3rd quartile","Maximum"), caption =
'\\label{tab:T4summary} Summary statistics on Iris variables.',
booktabs = TRUE, format = "latex")
Irs %>%
get_correlation(formula = Sepal.Width ~ Sepal.Length) %>%
kable(caption =
'\\label{tab:T4cor} Correlation Coefficient bewteen Sepal.Width and Sepal.Length',
booktabs = TRUE, format = "latex") %>%
kable_styling(font_size = 10, latex_options = "hold_position")
ggplot(Irs, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
geom_point() +
labs(x = "Sepal length (in centimetres)", y = "Sepal width (in centimetres)",
color = "Species") +
geom_smooth(method = "lm", se = FALSE)
int.model <- lm(Sepal.Width ~ Sepal.Length * Species, data = Irs)
get_regression_table(int.model) %>%
kable(caption =
'\\label{tab:T4reg} Estimated Coefficients from the fitted model
Sepal.Width = Sepal.Length . Species', booktabs = TRUE, format = "latex") %>%
kable_styling(font_size = 10, latex_options = "hold_position")
ggplot(regression.points, aes(x = Sepal.Length, y = residual)) +
geom_point() +
labs(x = "Sepal length (in centimetres)", y = "Residual") +
geom_hline(yintercept = 0, col = "blue", size = 1) +
facet_wrap(~ Species)
ggplot(regression.points, aes(x = Sepal.Length, y = residual)) +
geom_point() +
labs(x = "Sepal length (in centimetres)", y = "Residual") +
geom_hline(yintercept = 0, col = "blue", size = 1) +
facet_wrap(~ Species)
ggplot(regression.points, aes(x = Sepal.Length, y = residual)) +
geom_point()
regression.points <- get_regression_points(int.model)
Sepal.Length
regression.points
Sepal.Length
ggplot(regression.points, aes(x = Sepal.Length, y = residual)) +
geom_point() +
labs(x = "Sepal length (in centimetres)", y = "Residual") +
geom_hline(yintercept = 0, col = "blue", size = 1) +
facet_wrap(~ Species)
ggplot(regression.points, aes(x = Sepal.Width_hat, y = residual)) +
geom_point() +
labs(x = "Fitted values", y = "Residual") +
geom_hline(yintercept = 0, col = "blue", size = 1) +
facet_wrap(~ Species)
ggplot(regression.points, aes(x = residual)) +
geom_histogram(color = "white") +
labs(x = "Residual") +
facet_wrap(~ Species)
#apm_week_3
library(ggplot2)
library(class)
beetles <- read.csv(url("http://www.stats.gla.ac.uk/~tereza/rp/beetles.csv"))
beetles
beetles$propkilled <- beetles$killed / beetles$number
p1 <- ggplot(beetles, aes(x = dose, y = propkilled))+
geom_point(size = 1) + xlab ("Dose") + ylab ("Proportion killed")
#Since the response variable 푌푖 can only take one of two values (killed or not), we need to apply a generalised linear model. This takes the form 푔(푝푖) = 훽0 + 훽1푥푖 where 푥푖
#is the dose for  We start by considering the logit link function
m1 <- glm(cbind(killed, number-killed) ~ dose, family = binomial(link = 'logit'),
data=beetles)
summary(m1)
qchisq(df=1,p=0.95)
qchisq(df=6,p=0.95)
#For the logit model the fitted values can be obtained by taking the predicted probabilities, 푝ˆ푖
#, and multiplying them by the corresponding total number of beetles for 푖 = 1, . . . , 8:
p.hat=predict(m1,type="response")
fitted=beetles$number*p.hat
cbind(beetles$killed,round(fitted,2))
#model 2
m2 <- glm(cbind(killed, number-killed) ~ dose, family = binomial(link = 'probit'),
data=beetles)
summary(m2)
m3 <- glm(cbind(killed, number-killed) ~ dose, family = binomial(link = 'cloglog'),
data=beetles)
summary(m3)
beet_p <- data.frame(beetles = beetles,
logit = fitted(m1),
probit = fitted(m2),
cloglog = fitted(m3))
p2 <- ggplot(beet_p, aes(x = beetles$dose, y = beetles$propkilled)) +
geom_point() + xlab("Dose") + ylab("Proportion killed") +
geom_line(aes(x = beetles$dose, y = logit, colour = "Logit")) +
geom_line(aes(x = beetles$dose, y = probit, colour = "Probit")) +
geom_line(aes(x = beetles$dose, y = cloglog, colour = "C log-log")) +
guides(colour = guide_legend("Method"))
#example_2
install.packages("faraway")
library(faraway)
head(orings)
p1<- ggplot(orings, aes(x=temp, y=damage/6)) +
geom_point()+ xlim (c(25,85)) + ylim(c(0,1)) +
xlab ("Temperature (F)") + ylab("Probability of damage")
p1
# fit a binomial regression model to the data, trying out the logit,probit,
# and complementary log-log options for the link function
lmod=glm(cbind(damage,6-damage)~temp,family=binomial,data=orings)
summary(lmod)
pmod=glm(cbind(damage,6-damage)~temp,family=binomial(link="probit"),data=orings)
summary(lmod)
cmod=glm(cbind(damage,6-damage)~temp,family=binomial(link="cloglog"),data=orings)
summary(cmod)
#Task 2
#Superimpose the fitted probabilities from each of the three models on the above plot
?fitted
pred1=fitted(lmod)
pred1 <- predict(lmod, newdata=data.frame(temp=seq(25,85,length.out=23)), type="response")
pred2 <- predict(pmod, newdata=data.frame(temp=seq(25,85,length.out=23)), type="response")
pred3 <- predict(cmod, newdata=data.frame(temp=seq(25,85,length.out=23)), type="response")
pred=data.frame(logit=pred1,probit=pred2,cloglog=pred3,px=seq(25,85,le=23),orings)
p1.1 <- ggplot(pred, aes(x=orings$temp, y= orings$damage/6)) +
geom_point(size = 1)+ xlim (c(25,85)) + ylim(c(0,1)) +
xlab ("Temperature (F)") + ylab("Probability of damage") +
geom_line(aes(x = px, y = logit, color = "Logit")) +
geom_line(aes(x = px, y = probit, color = "Probit"))+
geom_line(aes(x = px, y = cloglog, color = "Complementary log-log"))
p1.1
#We can obtain the predicted probabilities using the model equation:
exp(11.6630-0.2162*31)/(1+exp(11.6630-0.2162*31))
#or
?predict
predict(lmod,newdata=data.frame(temp=31),type="response")
#Similarly, we can obtain the prediction for the probit model using the cumulative distribution function of a normal
#distribution:
pnorm(5.5915-0.1058*31)
#or by using the predict() function:
predict(pmod, newdata=data.frame(temp=31), type="response")
#Finally for the complementary log-log model the predicted probability is
predict(cmod, newdata=data.frame(temp=31), type="response")
yl<- read.csv(url("http://www.stats.gla.ac.uk/~tereza/rp/yl53.csv"))
head(yl)
yl$hear=factor(yl$hear)
yl.plot1=ggplot(yl,aes(y=age,x=hear))
?element_rect
yl.plot1 + geom_boxplot()+ xlab("What do you hear?") +
theme(panel.background = element_rect(fill = "transparent", colour = NA),
plot.background = element_rect(fill = "transparent", colour = NA),
panel.border = element_rect(fill = NA, colour = "black", size = 1))
install.packages("sjPlot")
library("sjPlot")
plot_xtab(yl$hear,yl$gender,show.values=F,show.total=F,
axis.labels = c("Laurel", "Yanny"),
axis.titles=c("What do you hear?"))
#logistic model
mod.yl=glm(hear~age,family=binomial,data=yl)
summary(mod.yl)
#Finally we can plot the predicted probabilities from this model as a function of age and, as expected, we see that the predicted probability of
#hearing “Yanny” decreases with age:
library(sjPlot)
plot_model(mod.yl,type="pred",terms=c("age"),axis.title=c("Age","Prob(hear Yanny)"),
title="",ci.lvl=NA)
#Task 4.
#Fit appropriate logistic regression models to explore if gender is related to whether people hear “Yanny”
#or “Laurel”.
mod.yl2 <- glm(hear ~ gender, family=binomial, data=yl)
summary(mod.yl2)
#or we can add gender to the model with age:
mod.yl3 <- glm(hear ~ gender+age, family=binomial, data=yl)
summary(mod.yl3)
##titanic
library(ggplot2)
library(sjPlot)
titanic=read.csv(url("http://www.stats.gla.ac.uk/~tereza/rp/titanic.csv"))
titanic$passenger.class=factor(titanic$passenger.class)
head(titanic)
??plot_xtab
plot_xtab(titanic$survived,titanic$gender, show.values = FALSE,
show.total = FALSE, axis.labels = c("Died", "Survived"),
legend.title = "Gender")
#There is a clear pattern here with the proportion surviving much higher for women than for men
plot_xtab(titanic$survived,titanic$passenger.class, show.values = FALSE,
show.total = FALSE, axis.labels = c("Died", "Survived"),
legend.title = "Class")
mod.titan=glm(survived~gender+passenger.class+age,
family=binomial(link="logit"),data=titanic)
summary(mod.titan)
#To quantify the effect of each of these predictors, we look at odds ratios which can be computed as exp(훽ˆ).
#These are shown in the plot below.
??plot_model
plot_model(mod.titan,show.values=T)
plot_model(mod.titan,type="pred",terms=c("age","passenger.class","gender"))
#We see the gender and class differences in survival we have already discussed, and also that survival
#mining W5
set.seed(147)
library(MASS)
library(e1071)
library(MASS)
#mining W5
set.seed(147)
library(MASS)
library(e1071)
data(crabs)
dim(crabs)
n<-nrow(crabs)
intrain<-sample(c(1:n),round(n/2))
n<-nrow(crabs)
intrain<-sample(c(1:n),round(n/2))
invalid<-sample((c(1:n)[-intrain]),round(n/4))
train.data<-crabs[intrain,-c(2:3)]
valid.data<-crabs[invalid,-c(2:3)]
test.data<-crabs[-c(intrain,invalid),-c(2:3)]
#Split the data 50% training, 25% validation and 25% test
n<-nrow(crabs)
intrain<-sample(c(1:n),round(n/2))
invalid<-sample((c(1:n)[-intrain]),round(n/4))
train.data<-crabs[intrain,-c(2:3)]
valid.data<-crabs[invalid,-c(2:3)]
test.data<-crabs[-c(intrain,invalid),-c(2:3)]
pred.error<-function(pred,truth){
mean(pred!=truth)
}
C.val <- c(0.1,0.5,1,2,5,10)
C.error <- numeric(length(C.val))
for (i in 1:length(C.val)) {
model <- svm(sp~.,data=train.data,type="C-classification",kernel="linear",
cost=C.val[i]) #kernel will be explained in the next section
pred.error<-function(pred,truth){
mean(pred!=truth)
}
C.val <- c(0.1,0.5,1,2,5,10)
C.error <- numeric(length(C.val))
for (i in 1:length(C.val)) {
model <- svm(sp~.,data=train.data,type="C-classification",kernel="linear",
cost=C.val[i]) #kernel will be explained in the next section
pred.model <- predict(model, valid.data)
C.error[i] <- pred.error(pred.model, valid.data$sp)
}
C.sel <- C.val[min(which.min(C.error))]
C.sel
pred.error<-function(pred,truth){
mean(pred!=truth)
}
C.val <- c(0.1,0.5,1,2,5,10)
C.error <- numeric(length(C.val))
for (i in 1:length(C.val)) {
model <- svm(sp~.,data=train.data,type="C-classification",kernel="linear",
cost=C.val[i]) #kernel will be explained in the next section
pred.model <- predict(model, valid.data)
C.error[i] <- pred.error(pred.model, valid.data$sp)
}
C.sel <- C.val[min(which.min(C.error))]
C.sel
plot(C.val,C.error,type="b")
abline(v=C.sel,lty=2)
#We can also use the tune.svm command to do something similar
#using cross validation on the training data.
tune.svm(sp~.,data=train.data,type="C-classification",kernel="linear",
cost=C.val)
final.svm<-svm(sp~.,data=train.data,kernel="linear",cost=C.sel,type="C-classification")
summary(final.svm)
final.svm<-svm(sp~.,data=train.data,kernel="linear",cost=C.sel,type="C-classification")
summary(final.svm)
final.svm<-svm(sp~.,data=train.data,kernel="linear",cost=C.sel,type="C-classification")
summary(final.svm)
pred.test<-predict(final.svm,test.data)
pred.error(pred.test,test.data$sp)
table(test.data$sp,pred.test)
final.svm$index
#mining_tut_2
setwd("/Users/kurisuuu/Documents/glasgow_stats_2021/semester\ 2/mining/Datasets\ for\ week\ 2-20210520")
letter <- read.csv("letter_small.csv",row.names=1)
letter
#convert into a dissimilarity matrix
library(smacof)
letter.dist <- sim2diss(letter,method=max(letter)+1) #z-s_ij
letter
letter <- read.csv("letter_small.csv",row.names=1)
#mining_tut_2
setwd("/Users/kurisuuu/Documents/glasgow_stats_2021/semester\ 2/mining/Datasets\ for\ week\ 2-20210520")
letter <- read.csv("letter_small.csv",row.names=1)
letter <- read.csv("letter.csv",row.names=1)
letter
letter
letter.dist
letter.dist <- sim2diss(letter,method=max(letter)+1) #z-s_ij
letter.dist
letter.dist <- as.dist(letter.dist)
letter.dist <- as.dist(letter.dist)
letter.dist
#apm_lab_1
## Load the package and data
library(faraway)
library(MASS)
data(orings)
## Fit a logistic model
logitmod <- glm(cbind(damage, 6-damage) ~ temp, family = binomial(), orings)
lsumm <- summary(logitmod)
lsumm
## Fit a probit model
probitmod <- glm(cbind(damage, 6-damage) ~ temp, family = binomial(link=probit), orings)
psumm <- summary(probitmod)
psumm
logitmod$coefficients[2]
lsumm$coefficients[2,2]
lsumm$coefficients
se.exp.beta1
## Compute the 95% confidence interval for exp(beta_1) using the delta method
se.exp.beta1 <- exp(logitmod$coefficients[2])*lsumm$coefficients[2,2]
se.exp.beta1
exp(logitmod$coefficients[2]) + c(-1, 1)*qnorm(0.975)*se.exp.beta1
logitmod$coefficients[2]
se.exp.beta1
c(-1, 1)*qnorm(0.975)*se.exp.beta1
exp(logitmod$coefficients[2]) + c(-1, 1)*qnorm(0.975)*se.exp.beta1
logitmod$coefficients[2]
c(-1, 1)*qnorm(0.975)
qnorm(0.975)
??outer
?outer
#3. Compute Wald and likelihood ratio confidence intervals for the model parameters
## Standard errors
se <- lsumm$coeff[, "Std. Error"]
## Compute the Wald confidence intervals (95% confidence)
logitmod$coeff + qnorm(0.975)*outer(se, c(-1,1))
confint(logitmod)
#4. Show how to use the likelihood ratio test for comparing the logistic model and the intercept-only model.
G=logitmod$null.deviance-logitmod$deviance
G
pchisq(G,1,lower.taill=F)
pchisq(G, 1, lower.tail = F)
#or using anova table
## Fit the intercept-only model
nullmod <- glm(cbind(damage, 6-damage) ~ 1, family=binomial(), data=orings)
## Compare the null model and the logistic model
anova(nullmod, logitmod, test="LRT")
#5. Use deviance to determine if the logistic model fits the data well
pchisq(logitmod$deviance, logitmod$df.residual, lower.tail=F)
logitmod$deviance
logitmod$df.residual
?dchisq
#6. Compare the logistic model and the probit model for this dataset.
## AIC values for the logistic model and the probit model
c(logitmod$aic, probitmod$aic)
#7. Add fitted lines to the original plot
## Explore raw data
plot(damage/6 ~ temp, orings, xlim=c(25,85), ylim=c(0, 1), xlab="Temperature",
ylab="Prob of damage")
x <- seq(25, 85, 1)
## Logistic
lines(x, ilogit(logitmod$coefficients[1]+logitmod$coefficients[2]*x), lty=1)
## Probit
lines(x, pnorm(probitmod$coefficients[1]+probitmod$coefficients[2]*x), lty=2)
